# CAFA 6 Protein Function Prediction - Team 0165_INT34057

## ğŸ‘¥ ThÃ nh viÃªn nhÃ³m
* **LÃª Trá»ng Thá»±c** - MSSV: 23020165
---

## ğŸ“– Giá»›i thiá»‡u
Dá»± Ã¡n nÃ y sá»­ dá»¥ng chiáº¿n lÆ°á»£c **Ensemble 3 luá»“ng (Three-stream Ensemble)** Ä‘á»ƒ dá»± Ä‘oÃ¡n chá»©c nÄƒng protein (Gene Ontology). NhÃ³m káº¿t há»£p 3 kiáº¿n trÃºc mÃ´ hÃ¬nh há»c sÃ¢u khÃ¡c nhau Ä‘á»ƒ tá»‘i Æ°u hÃ³a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a.

### ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng
```mermaid
graph LR
    A[Input Data] --> B(ESM-2 Embeddings)
    A --> C(Sequence One-Hot)
    A --> D(ProtBERT Embeddings)

    B --> E[Model 1: MLP]
    C --> F[Model 2: 1D-CNN]
    D --> G[Model 3: Transformer/BLAST]

    E --> H(Score 1)
    F --> I(Score 2)
    G --> J(Score 3)

    H --> K{Ensemble Blending}
    I --> K
    J --> K

    K --> L[Final Submission]
    style K fill:#007bff,stroke:#333,stroke-width:2px,color:#fff
```
### ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n
- `1_train_models/`: Chá»©a 3 notebook Ä‘áº¡i diá»‡n cho 3 phÆ°Æ¡ng phÃ¡p Ä‘á»™c láº­p.
  - `01_ESM2_MLP.ipynb`: MÃ´ hÃ¬nh Deep Learning dá»±a trÃªn ESM-2 Embeddings (Baseline).
  - `02_1D_CNN.ipynb`: MÃ´ hÃ¬nh CNN 1 chiá»u Ä‘á»ƒ báº¯t cÃ¡c motif cá»¥c bá»™.
  - `03_ProtBERT.ipynb`: MÃ´ hÃ¬nh dá»±a trÃªn ProtBERT Embeddings.
- `2_ensemble/`: Chá»©a mÃ£ nguá»“n Ä‘á»ƒ gá»™p káº¿t quáº£.
  - `04_Ensemble_Blending.ipynb`: TÃ­nh trung bÃ¬nh cÃ³ trá»ng sá»‘ tá»« 3 káº¿t quáº£ trÃªn.
- `output/`: ThÆ° má»¥c chá»©a cÃ¡c file submission (khÃ´ng upload lÃªn GitHub Ä‘á»ƒ tiáº¿t kiá»‡m dung lÆ°á»£ng).

## ğŸš€ HÆ°á»›ng dáº«n cháº¡y (Workflow)
Äá»ƒ tÃ¡i láº­p káº¿t quáº£, cáº§n cháº¡y theo thá»© tá»± sau:

### 1. **CÃ i Ä‘áº·t thÆ° viá»‡n:**
   ```bash
   pip install -r requirements.txt
   ```
### 2. Giai Ä‘oáº¡n 1: Huáº¥n luyá»‡n & Dá»± Ä‘oÃ¡n Ä‘á»™c láº­p
Trong giai Ä‘oáº¡n nÃ y, nhÃ³m cháº¡y 3 notebook riÃªng biá»‡t Ä‘á»ƒ táº¡o ra 3 bá»™ dá»± Ä‘oÃ¡n (submission files) khÃ¡c nhau. Má»—i notebook Ä‘áº¡i diá»‡n cho má»™t phÆ°Æ¡ng phÃ¡p tiáº¿p cáº­n Ä‘áº·c thÃ¹.

#### ğŸ”¹ Model 1: ESM-2 + MLP (Baseline)
* **Notebook:** `1_train_models/01_ESM2_MLP.ipynb`
* **MÃ´ táº£:** Sá»­ dá»¥ng embeddings tá»« mÃ´ hÃ¬nh ngÃ´n ngá»¯ **ESM-2** (Meta AI) káº¿t há»£p vá»›i máº¡ng nÆ¡-ron truyá»n tháº³ng (MLP - Multi Layer Perceptron) 3 lá»›p áº©n [1024, 512, 256].
* **Dá»¯ liá»‡u Ä‘áº§u vÃ o:** `cafa-5-ems-2-embeddings-numpy`
* **Äáº§u ra:** File `output/submission_1.tsv`
* **Äiá»ƒm Ä‘áº·c biá»‡t:** Náº¯m báº¯t tá»‘t ngá»¯ nghÄ©a toÃ n cá»¥c cá»§a protein, Ä‘Ã³ng vai trÃ² lÃ  "xÆ°Æ¡ng sá»‘ng" cho há»‡ thá»‘ng Ensemble.

#### ğŸ”¹ Model 2: 1D-CNN (Local Motifs)
* **Notebook:** `1_train_models/02_1D_CNN.ipynb`
* **MÃ´ táº£:** Sá»­ dá»¥ng máº¡ng tÃ­ch cháº­p 1 chiá»u (**1D-Convolutional Neural Network**) Ä‘á»ƒ quÃ©t qua chuá»—i protein.
* **Dá»¯ liá»‡u Ä‘áº§u vÃ o:** TÆ°Æ¡ng thÃ­ch vá»›i ESM-2 hoáº·c ProtBERT embeddings.
* **Äáº§u ra:** File `output/submission_2.tsv`
* **Äiá»ƒm Ä‘áº·c biá»‡t:** MÃ´ hÃ¬nh nÃ y chuyÃªn biá»‡t trong viá»‡c phÃ¡t hiá»‡n cÃ¡c **motif cá»¥c bá»™** (cÃ¡c Ä‘oáº¡n trÃ¬nh tá»± ngáº¯n láº·p láº¡i cÃ³ chá»©c nÄƒng sinh há»c) mÃ  mÃ´ hÃ¬nh MLP cÃ³ thá»ƒ bá» sÃ³t.

#### ğŸ”¹ Model 3: ProtBERT / Multi-Embedding
* **Notebook:** `1_train_models/03_ProtBERT.ipynb`
* **MÃ´ táº£:** Sá»­ dá»¥ng embeddings tá»« **ProtBERT** (Google) hoáº·c T5 Ä‘á»ƒ Ä‘a dáº¡ng hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng.
* **Dá»¯ liá»‡u Ä‘áº§u vÃ o:** `protbert-embeddings-for-cafa5` hoáº·c `t5embeds`.
* **Äáº§u ra:** File `output/submission_3.tsv`
* **Äiá»ƒm Ä‘áº·c biá»‡t:** Cung cáº¥p gÃ³c nhÃ¬n khÃ¡c vá» dá»¯ liá»‡u (Representation Diversity), giÃºp giáº£m thiá»ƒu sai sá»‘ khi ensemble vá»›i ESM-2.

---

### 3. Giai Ä‘oáº¡n 2: Ensemble (Káº¿t há»£p káº¿t quáº£)
ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng nháº¥t Ä‘á»ƒ tá»•ng há»£p sá»©c máº¡nh cá»§a cáº£ 3 mÃ´ hÃ¬nh trÃªn.

* **Notebook:** `2_ensemble/04_Ensemble_Blending.ipynb`
* **PhÆ°Æ¡ng phÃ¡p:** Weighted Averaging (Trung bÃ¬nh cá»™ng cÃ³ trá»ng sá»‘).
* **Äáº§u vÃ o:** 3 file `submission_*.tsv` tá»« Giai Ä‘oáº¡n 1.
* **CÃ´ng thá»©c:**
    
    $$Score_{Final} = (w_1 \times Score_{MLP}) + (w_2 \times Score_{CNN}) + (w_3 \times Score_{ProtBERT})$$
    
    *Trong Ä‘Ã³ bá»™ trá»ng sá»‘ Ä‘Æ°á»£c thiáº¿t láº­p lÃ :*
    * $w_1 = 0.5$ (ESM-2 - MÃ´ hÃ¬nh tá»‘t nháº¥t)
    * $w_2 = 0.3$ (1D-CNN)
    * $w_3 = 0.2$ (ProtBERT)

* **CÃ¡ch cháº¡y:**
    1. Äáº£m báº£o Ä‘Ã£ cÃ³ Ä‘á»§ 3 file submission trong thÆ° má»¥c `output/`.
    2. Cháº¡y notebook `04_Ensemble_Blending.ipynb`.
    3. Káº¿t quáº£ cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c lÆ°u táº¡i: `output/final_submission.tsv`.

---
**âš ï¸ LÆ°u Ã½ vá» dá»¯ liá»‡u:**
CÃ¡c notebook trÃªn yÃªu cáº§u bá»™ dá»¯ liá»‡u embeddings ráº¥t lá»›n. Náº¿u cháº¡y trÃªn mÃ¡y cÃ¡ nhÃ¢n, cáº§n táº£i cÃ¡c dataset sau tá»« Kaggle vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `input/` (cáº§n cáº¥u hÃ¬nh láº¡i Ä‘Æ°á»ng dáº«n trong code náº¿u khÃ¡c biá»‡t):
1. `cafa-5-ems-2-embeddings-numpy`
2. `protbert-embeddings-for-cafa5`
3. `t5embeds`