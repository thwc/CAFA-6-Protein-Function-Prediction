{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Ghi ch√∫ t·ªëi ∆∞u t√†i nguy√™n**\n",
    "\n",
    "ƒê·ªÉ tr√°nh qu√° t·∫£i VRAM (MX550 ~2GB) v√† gi·∫£m th·ªùi gian train, notebook n√†y √°p d·ª•ng c√°c c·∫Øt gi·∫£m sau:\n",
    "- Gi·∫£m s·ªë nh√£n t·ª´ **1500 ‚Üí 500**\n",
    "- Gi·∫£m **batch size t·ª´ 128 ‚Üí 32**\n",
    "- Gi·∫£m s·ªë **epochs t·ª´ 22 ‚Üí 4 m·ªói fold**\n",
    "- (T√πy ch·ªçn) gi·∫£m s·ªë **fold t·ª´ 5 ‚Üí 3**\n",
    "\n",
    "C√°c thay ƒë·ªïi n√†y gi√∫p gi·∫£m ƒë√°ng k·ªÉ chi ph√≠ t√≠nh to√°n (**‚âà3‚Äì5√ó**), **ƒë·ªïi l·∫°i ƒë·ªô ch√≠nh x√°c gi·∫£m r√µ r·ªát**: *public score tr√™n Kaggle gi·∫£m t·ª´ **0.226** (c·∫•u h√¨nh g·ªëc) xu·ªëng c√≤n **~0.187*** v·ªõi c·∫•u h√¨nh r√∫t g·ªçn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: E:\\CAFA-6-Protein-Function-Prediction\n",
      "üìÇ Input: E:\\CAFA-6-Protein-Function-Prediction\\input\n",
      "üìÇ Output: E:\\CAFA-6-Protein-Function-Prediction\\output\n",
      "üìÇ Models: E:\\CAFA-6-Protein-Function-Prediction\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH G·ªêC (ROOT) ---\n",
    "# D√πng r'' ƒë·ªÉ Python hi·ªÉu ƒë√¢y l√† ƒë∆∞·ªùng d·∫´n Windows (kh√¥ng b·ªã l·ªói k√Ω t·ª± ƒë·∫∑c bi·ªát)\n",
    "ROOT = r'E:\\CAFA-6-Protein-Function-Prediction'\n",
    "\n",
    "# --- 2. ƒê·ªäNH NGHƒ®A C√ÅC TH∆Ø M·ª§C CON (T·ª± ƒë·ªông n·ªëi ƒëu√¥i) ---\n",
    "# os.path.join gi√∫p n·ªëi ƒë∆∞·ªùng d·∫´n ƒë√∫ng chu·∫©n cho c·∫£ Windows/Linux/Mac\n",
    "INPUT_DIR  = os.path.join(ROOT, 'input')\n",
    "OUTPUT_DIR = os.path.join(ROOT, 'output')\n",
    "MODEL_DIR  = os.path.join(ROOT, 'models')\n",
    "\n",
    "# T·∫°o s·∫µn th∆∞ m·ª•c output v√† models n·∫øu ch∆∞a c√≥ (Tr√°nh l·ªói kh√¥ng l∆∞u ƒë∆∞·ª£c file)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: {ROOT}\")\n",
    "print(f\"üìÇ Input: {INPUT_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìÇ Models: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h√†m t√≠nh fmax\n",
    "import numpy as np\n",
    "def calculate_fmax(preds, targets):\n",
    "    \"\"\"\n",
    "    T√≠nh F-max Score v√† t√¨m ng∆∞·ª°ng (threshold) t·ªët nh·∫•t.\n",
    "    preds: numpy array (N_samples, N_labels) - x√°c su·∫•t d·ª± ƒëo√°n (sau sigmoid)\n",
    "    targets: numpy array (N_samples, N_labels) - nh√£n th·∫≠t (0 ho·∫∑c 1)\n",
    "    \"\"\"\n",
    "    # Th·ª≠ c√°c ng∆∞·ª°ng t·ª´ 0.01 ƒë·∫øn 0.99\n",
    "    thresholds = np.linspace(0.01, 1.0, 50)\n",
    "    fmax = 0.0\n",
    "    best_t = 0.0\n",
    "\n",
    "    # Chuy·ªÉn target v·ªÅ int ƒë·ªÉ t√≠nh to√°n\n",
    "    targets = targets.astype(int)\n",
    "\n",
    "    for t in thresholds:\n",
    "        # √Åp d·ª•ng ng∆∞·ª°ng t ƒë·ªÉ t·∫°o nh√£n d·ª± ƒëo√°n (0 ho·∫∑c 1)\n",
    "        p_cut = (preds >= t).astype(int)\n",
    "\n",
    "        # T√≠nh TP, FP, FN cho t·ª´ng m·∫´u (axis=1)\n",
    "        tp = (p_cut * targets).sum(axis=1)\n",
    "        fp = (p_cut * (1 - targets)).sum(axis=1)\n",
    "        fn = ((1 - p_cut) * targets).sum(axis=1)\n",
    "\n",
    "        # T√≠nh Precision v√† Recall cho t·ª´ng m·∫´u\n",
    "        # Tr√°nh chia cho 0: n·∫øu tp+fp=0 -> precision=0; n·∫øu tp+fn=0 -> recall=0\n",
    "        precision = np.divide(tp, (tp + fp), out=np.zeros_like(tp, dtype=float), where=(tp + fp) != 0)\n",
    "        recall = np.divide(tp, (tp + fn), out=np.zeros_like(tp, dtype=float), where=(tp + fn) != 0)\n",
    "\n",
    "        # T√≠nh F1 trung b√¨nh tr√™n to√†n b·ªô t·∫≠p m·∫´u (Average F1)\n",
    "        p_avg = precision.mean()\n",
    "        r_avg = recall.mean()\n",
    "        \n",
    "        if (p_avg + r_avg) > 0:\n",
    "            f1 = 2 * p_avg * r_avg / (p_avg + r_avg)\n",
    "        else:\n",
    "            f1 = 0\n",
    "\n",
    "        if f1 > fmax:\n",
    "            fmax = f1\n",
    "            best_t = t\n",
    "\n",
    "    return fmax, best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:55:13.680480Z",
     "iopub.status.busy": "2025-12-15T13:55:13.679700Z",
     "iopub.status.idle": "2025-12-15T13:55:20.532817Z",
     "shell.execute_reply": "2025-12-15T13:55:20.532045Z",
     "shell.execute_reply.started": "2025-12-15T13:55:13.680441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Global Seed: 42\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import & Config & Seeding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    print(f\"Set Global Seed: {seed}\")\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "INPUT_EMBED_DIR = f\"{INPUT_DIR}/cafa6-t5-embeddings\"\n",
    "\n",
    "CONFIG = {\n",
    "    #\"num_labels\": 1500,\n",
    "    \"num_labels\": 500,\n",
    "    #\"batch_size\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.0007,      \n",
    "    #\"epochs\": 22,    \n",
    "    \"epochs\": 4,\n",
    "    \"seed\": SEED,\n",
    "    \"paths\": {\n",
    "        \"train_embeds\": f\"{INPUT_EMBED_DIR}/train_embeddings_esm2.npy\",\n",
    "        \"train_ids\":    f\"{INPUT_EMBED_DIR}/train_ids_esm2.npy\",\n",
    "        \"test_embeds\":  f\"{INPUT_EMBED_DIR}/test_embeddings_esm2.npy\",\n",
    "        \"test_ids\":     f\"{INPUT_EMBED_DIR}/test_ids_esm2.npy\",\n",
    "        \"train_terms\":  f\"{INPUT_DIR}/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:55:20.534688Z",
     "iopub.status.busy": "2025-12-15T13:55:20.534243Z",
     "iopub.status.idle": "2025-12-15T13:55:31.499443Z",
     "shell.execute_reply": "2025-12-15T13:55:31.498507Z",
     "shell.execute_reply.started": "2025-12-15T13:55:20.534667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from disk...\n",
      "--- Data Loaded ---\n",
      "Train shape: (82404, 1280)\n",
      "Test shape:  (224309, 1280)\n",
      "Embedding Dimension detected: 1280\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Embeddings\n",
    "print(\"Loading embeddings from disk...\")\n",
    "\n",
    "# Load Train\n",
    "train_embeds = np.load(CONFIG['paths']['train_embeds'])\n",
    "train_ids = np.load(CONFIG['paths']['train_ids'], allow_pickle=True)\n",
    "\n",
    "# Load Test\n",
    "test_embeds = np.load(CONFIG['paths']['test_embeds'])\n",
    "test_ids = np.load(CONFIG['paths']['test_ids'], allow_pickle=True)\n",
    "\n",
    "# T·ª± ƒë·ªông c·∫≠p nh·∫≠t k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o (Input Dimension)\n",
    "# Gi√∫p code ch·∫°y ƒë√∫ng d√π vector l√† T5 (1024) hay ESM2 (1280/2560)\n",
    "EMBED_DIM = train_embeds.shape[1]\n",
    "\n",
    "print(f\"--- Data Loaded ---\")\n",
    "print(f\"Train shape: {train_embeds.shape}\")\n",
    "print(f\"Test shape:  {test_embeds.shape}\")\n",
    "print(f\"Embedding Dimension detected: {EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:55:31.500818Z",
     "iopub.status.busy": "2025-12-15T13:55:31.500406Z",
     "iopub.status.idle": "2025-12-15T13:55:32.569405Z",
     "shell.execute_reply": "2025-12-15T13:55:32.568768Z",
     "shell.execute_reply.started": "2025-12-15T13:55:31.500769Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Targets & Fixing ID Format ---\n",
      "[DEBUG] ID g·ªëc: sp|A0A0C5B5G6|MOTSC_HUMAN\n",
      "[DEBUG] ID sau khi s·ª≠a: A0A0C5B5G6\n",
      "S·ªë l∆∞·ª£ng d√≤ng kh·ªõp ƒë∆∞·ª£c: 264886\n",
      "ƒêang t·∫°o ma tr·∫≠n nh√£n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 264886/264886 [00:00<00:00, 1039530.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TH√ÄNH C√îNG! T·ªïng s·ªë nh√£n d∆∞∆°ng: 264886.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Prepare Targets (S·ª¨A L·ªñI ƒê·ªäNH D·∫†NG ID: sp|ID|NAME -> ID)\n",
    "import gc\n",
    "\n",
    "print(\"--- Processing Targets & Fixing ID Format ---\")\n",
    "\n",
    "# 1. Load d·ªØ li·ªáu\n",
    "train_terms = pd.read_csv(CONFIG['paths']['train_terms'], sep=\"\\t\")\n",
    "train_ids = np.load(CONFIG['paths']['train_ids'], allow_pickle=True)\n",
    "\n",
    "# --- S·ª¨A L·ªñI LOGIC T·∫†I ƒê√ÇY ---\n",
    "def clean_id(pid):\n",
    "    # Chuy·ªÉn bytes sang string n·∫øu c·∫ßn\n",
    "    if isinstance(pid, bytes):\n",
    "        pid = pid.decode('utf-8')\n",
    "    pid_str = str(pid).strip()\n",
    "    \n",
    "    # Logic t√°ch chu·ªói: \"sp|A0A0C5B5G6|MOTSC_HUMAN\" -> l·∫•y \"A0A0C5B5G6\"\n",
    "    # T√°ch b·∫±ng d·∫•u g·∫°ch ƒë·ª©ng '|' v√† l·∫•y ph·∫ßn t·ª≠ th·ª© 2 (index 1)\n",
    "    parts = pid_str.split('|')\n",
    "    if len(parts) > 1:\n",
    "        return parts[1] # L·∫•y m√£ ·ªü gi·ªØa\n",
    "    return pid_str # Tr·∫£ v·ªÅ nguy√™n g·ªëc n·∫øu kh√¥ng t√¨m th·∫•y d·∫•u |\n",
    "\n",
    "# √Åp d·ª•ng h√†m s·ª≠a l·ªói\n",
    "train_ids_clean = [clean_id(pid) for pid in train_ids]\n",
    "\n",
    "print(f\"[DEBUG] ID g·ªëc: {train_ids[0]}\")\n",
    "print(f\"[DEBUG] ID sau khi s·ª≠a: {train_ids_clean[0]}\") # Mong ƒë·ª£i: A0A0C5B5G6\n",
    "\n",
    "# T·∫°o map\n",
    "id_map = {pid: i for i, pid in enumerate(train_ids_clean)}\n",
    "\n",
    "# 2. Ch·ªçn Top Labels\n",
    "top_terms = train_terms['term'].value_counts().index[:CONFIG['num_labels']]\n",
    "term_to_idx = {term: i for i, term in enumerate(top_terms)}\n",
    "\n",
    "# 3. T·∫°o ma tr·∫≠n Targets\n",
    "num_samples = len(train_ids)\n",
    "labels_matrix = np.zeros((num_samples, CONFIG['num_labels']), dtype=np.float32)\n",
    "\n",
    "# L·ªçc d·ªØ li·ªáu\n",
    "train_terms['EntryID'] = train_terms['EntryID'].astype(str).str.strip()\n",
    "filtered_terms = train_terms[\n",
    "    (train_terms['EntryID'].isin(id_map)) & \n",
    "    (train_terms['term'].isin(top_terms))\n",
    "]\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng d√≤ng kh·ªõp ƒë∆∞·ª£c: {len(filtered_terms)}\")\n",
    "\n",
    "if len(filtered_terms) == 0:\n",
    "    print(\"Error\")\n",
    "else:\n",
    "    print(\"ƒêang t·∫°o ma tr·∫≠n nh√£n...\")\n",
    "    for pid, term in tqdm(zip(filtered_terms['EntryID'], filtered_terms['term']), total=len(filtered_terms)):\n",
    "        row_idx = id_map[pid]\n",
    "        col_idx = term_to_idx[term]\n",
    "        labels_matrix[row_idx, col_idx] = 1.0\n",
    "        \n",
    "    print(f\"TH√ÄNH C√îNG! T·ªïng s·ªë nh√£n d∆∞∆°ng: {labels_matrix.sum()}\")\n",
    "\n",
    "del train_terms, filtered_terms, train_ids_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:55:32.571039Z",
     "iopub.status.busy": "2025-12-15T13:55:32.570739Z",
     "iopub.status.idle": "2025-12-15T13:55:32.587566Z",
     "shell.execute_reply": "2025-12-15T13:55:32.586915Z",
     "shell.execute_reply.started": "2025-12-15T13:55:32.571020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Enhanced Hybrid Model\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, embeddings, targets=None, ids=None):\n",
    "        self.embeddings = embeddings\n",
    "        self.targets = targets\n",
    "        self.ids = ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embed = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        if self.targets is not None:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return embed, target\n",
    "        return embed, self.ids[idx]\n",
    "\n",
    "class ResidualCNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.act = nn.GELU() # ƒê·ªïi sang GELU m∆∞·ª£t h∆°n\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(self.bn2(self.conv2(out)))\n",
    "        out += residual\n",
    "        return self.act(out)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c = x.size()\n",
    "        y = x.view(b, c, 1)\n",
    "        y = self.avg_pool(y).view(b, c)\n",
    "        y = self.fc(y)\n",
    "        return x * y\n",
    "\n",
    "class DenseResBlock(nn.Module):\n",
    "    def __init__(self, features, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(features, features * 2), # Expand width\n",
    "            nn.BatchNorm1d(features * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(features * 2, features), # Project back\n",
    "            nn.BatchNorm1d(features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.se = SEBlock(features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.se(self.block(x)) + x # Residual connection\n",
    "\n",
    "class HybridSystem(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Nh√°nh Residual CNN (S√¢u h∆°n, gi·ªØ feature t·ªët h∆°n)\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.GELU(),\n",
    "            ResidualCNNBlock(64, 128, kernel_size=5),\n",
    "            nn.MaxPool1d(2),\n",
    "            ResidualCNNBlock(128, 256, kernel_size=3),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * (input_dim // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 2. Nh√°nh Wide ResNet (M·ªü r·ªông chi·ªÅu ngang)\n",
    "        self.resnet_branch = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            DenseResBlock(1024, dropout_rate=0.2),\n",
    "            DenseResBlock(1024, dropout_rate=0.2),\n",
    "            DenseResBlock(1024, dropout_rate=0.2), # Th√™m 1 block n·ªØa\n",
    "        )\n",
    "        \n",
    "        # 3. Fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(1024 + 1024, 1536),\n",
    "            nn.BatchNorm1d(1536),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Multi-Sample Dropout\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.4) for _ in range(5)])\n",
    "        self.fc = nn.Linear(1536, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN Branch\n",
    "        x_cnn = x.unsqueeze(1)\n",
    "        out_cnn = self.cnn_branch(x_cnn)\n",
    "        \n",
    "        # ResNet Branch\n",
    "        out_res = self.resnet_branch(x)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([out_cnn, out_res], dim=1)\n",
    "        features = self.fusion(combined)\n",
    "        \n",
    "        # Output\n",
    "        output = torch.zeros(features.size(0), self.fc.out_features).to(features.device)\n",
    "        for dropout in self.dropouts:\n",
    "            output += self.fc(dropout(features))\n",
    "        return output / len(self.dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:55:32.588590Z",
     "iopub.status.busy": "2025-12-15T13:55:32.588304Z",
     "iopub.status.idle": "2025-12-15T14:57:28.283358Z",
     "shell.execute_reply": "2025-12-15T14:57:28.282200Z",
     "shell.execute_reply.started": "2025-12-15T13:55:32.588573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START TRAINING (ASYMMETRIC LOSS + RESIDUAL MODEL) ---\n",
      "\n",
      ">>> FOLD 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 108\u001b[39m\n\u001b[32m    106\u001b[39m     optimizer.step()\n\u001b[32m    107\u001b[39m     scheduler.step()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m model.eval()\n\u001b[32m    111\u001b[39m val_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 5: Training with Asymmetric Loss\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "# --- ASYMMETRIC LOSS ---\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: logits, y: targets (0 or 1)\n",
    "        \n",
    "        # T√≠nh sigmoid probabilities\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        x_sigmoid_pos = x_sigmoid\n",
    "        x_sigmoid_neg = 1 - x_sigmoid\n",
    "\n",
    "        # Asymmetric Clipping \n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            x_sigmoid_neg = (x_sigmoid_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # Basic Cross Entropy parts\n",
    "        loss_pos = y * torch.log(x_sigmoid_pos.clamp(min=self.eps))\n",
    "        loss_neg = (1 - y) * torch.log(x_sigmoid_neg.clamp(min=self.eps))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            \n",
    "            pt0 = x_sigmoid_pos * y\n",
    "            pt1 = x_sigmoid_neg * (1 - y)  # pt = p if t=1 else 1-p\n",
    "            pt = pt0 + pt1\n",
    "\n",
    "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            \n",
    "            loss_pos *= one_sided_w\n",
    "            loss_neg *= one_sided_w\n",
    "\n",
    "        return -torch.mean(loss_pos + loss_neg)\n",
    "\n",
    "# --- CONFIG TRAINING ---\n",
    "N_FOLDS = 5\n",
    "#EPOCHS_PER_FOLD = 20\n",
    "EPOCHS_PER_FOLD = CONFIG['epochs']\n",
    "#LR = 0.001\n",
    "LR = CONFIG['lr']\n",
    "#BATCH_SIZE = 128\n",
    "BATCH_SIZE = CONFIG['batch_size']\n",
    "WEIGHT_DECAY = 1e-2 \n",
    "\n",
    "kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=CONFIG['seed'])\n",
    "print(f\"\\n--- START TRAINING (ASYMMETRIC LOSS + RESIDUAL MODEL) ---\")\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_embeds, labels_matrix)):\n",
    "    print(f\"\\n>>> FOLD {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    # Load Data\n",
    "    X_train, X_val = train_embeds[train_idx], train_embeds[val_idx]\n",
    "    y_train, y_val = labels_matrix[train_idx], labels_matrix[val_idx]\n",
    "    \n",
    "    train_dataset = ProteinDataset(X_train, y_train)\n",
    "    val_dataset = ProteinDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "    \n",
    "    # Init Model\n",
    "    model = HybridSystem(input_dim=EMBED_DIM, num_classes=CONFIG['num_labels']).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    criterion = AsymmetricLossOptimized(gamma_neg=4, gamma_pos=1, clip=0.05)\n",
    "    \n",
    "    scheduler = OneCycleLR(optimizer, max_lr=LR, steps_per_epoch=len(train_loader), \n",
    "                           epochs=EPOCHS_PER_FOLD, pct_start=0.3)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(EPOCHS_PER_FOLD):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for embeds, targets in tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "            embeds, targets = embeds.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeds)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (V·∫´n gi·ªØ ƒë·ªÉ an to√†n)\n",
    "            utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds_all = []\n",
    "        val_targets_all = []\n",
    "        with torch.no_grad():\n",
    "            for embeds, targets in val_loader:\n",
    "                embeds, targets = embeds.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(embeds)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_preds_all.append(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "                val_targets_all.append(targets.detach().cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # --- TH√äM ƒêO·∫†N T√çNH F-MAX N√ÄY ---\n",
    "        # G·ªôp c√°c batch l·∫°i\n",
    "        val_preds_concat = np.concatenate(val_preds_all, axis=0)\n",
    "        val_targets_concat = np.concatenate(val_targets_all, axis=0)\n",
    "        \n",
    "        # G·ªçi h√†m t√≠nh\n",
    "        val_fmax, val_best_t = calculate_fmax(val_preds_concat, val_targets_concat)\n",
    "        # --------------------------------\n",
    "\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f\"model_fold_{fold}.pth\")\n",
    "            # --- S·ª¨A PRINT C√ì NEW BEST ---\n",
    "            print(f\"Ep {epoch+1}: Loss={avg_val_loss:.5f} | F-max={val_fmax:.4f} (t={val_best_t:.2f}) ‚≠ê NEW BEST\")\n",
    "        else:\n",
    "            # --- S·ª¨A PRINT TH∆Ø·ªúNG ---\n",
    "            print(f\"Ep {epoch+1}: Loss={avg_val_loss:.5f} | F-max={val_fmax:.4f} (t={val_best_t:.2f})\")\n",
    "            \n",
    "    # --- S·ª¨A D√íNG IN K·∫æT QU·∫¢ FOLD ---\n",
    "    print(f\"Fold {fold+1} Best ASL Loss: {best_loss:.5f} (Last F-max: {val_fmax:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:57:28.285319Z",
     "iopub.status.busy": "2025-12-15T14:57:28.284678Z",
     "iopub.status.idle": "2025-12-15T15:05:26.846097Z",
     "shell.execute_reply": "2025-12-15T15:05:26.845304Z",
     "shell.execute_reply.started": "2025-12-15T14:57:28.285286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 6: Ensemble Prediction (Hybrid System)\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n--- Generating Predictions (Hybrid System) ---\")\n",
    "\n",
    "# 1. Kh√¥i ph·ª•c mapping\n",
    "print(\"Re-creating index mapping...\")\n",
    "df_terms = pd.read_csv(CONFIG['paths']['train_terms'], sep=\"\\t\")\n",
    "top_terms = df_terms['term'].value_counts().index[:CONFIG['num_labels']]\n",
    "idx_to_term = {i: term for i, term in enumerate(top_terms)}\n",
    "del df_terms, top_terms\n",
    "gc.collect()\n",
    "\n",
    "# 2. Setup Data Loader\n",
    "BATCH_SIZE = 256\n",
    "test_dataset_final = ProteinDataset(test_embeds, ids=test_ids)\n",
    "test_loader_final = DataLoader(test_dataset_final, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 3. Load Models\n",
    "models = []\n",
    "for fold in range(N_FOLDS):\n",
    "    # D√πng ƒë√∫ng class HybridSystem\n",
    "    model = HybridSystem(input_dim=EMBED_DIM, num_classes=CONFIG['num_labels']).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"model_fold_{fold}.pth\"))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "print(f\"Loaded {len(models)} Hybrid models.\")\n",
    "\n",
    "# 4. Prediction\n",
    "THRESHOLD = 0.02\n",
    "TOP_K = 75       \n",
    "output_file = f\"{OUTPUT_DIR}\\submission_hybrid.tsv\"\n",
    "\n",
    "print(f\"Inference... Threshold={THRESHOLD}, Top-K={TOP_K}\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    with torch.no_grad():\n",
    "        for step, (embeds, batch_ids) in enumerate(tqdm(test_loader_final)):\n",
    "            embeds = embeds.to(DEVICE)\n",
    "            \n",
    "            avg_probs = None\n",
    "            for model in models:\n",
    "                logits = model(embeds)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                if avg_probs is None: avg_probs = probs\n",
    "                else: avg_probs += probs\n",
    "            \n",
    "            avg_probs /= len(models)\n",
    "            avg_probs = avg_probs.cpu().numpy()\n",
    "            \n",
    "            batch_lines = []\n",
    "            for i, pid in enumerate(batch_ids):\n",
    "                row_probs = avg_probs[i]\n",
    "                idx_candidates = np.where(row_probs > THRESHOLD)[0]\n",
    "                \n",
    "                if len(idx_candidates) > TOP_K:\n",
    "                    candidate_probs = row_probs[idx_candidates]\n",
    "                    final_indices = idx_candidates[np.argsort(candidate_probs)[-TOP_K:]]\n",
    "                else:\n",
    "                    final_indices = idx_candidates\n",
    "                \n",
    "                for idx in final_indices:\n",
    "                    batch_lines.append(f\"{pid}\\t{idx_to_term[idx]}\\t{row_probs[idx]:.3f}\\n\")\n",
    "            \n",
    "            f.writelines(batch_lines)\n",
    "            del batch_lines, avg_probs, embeds\n",
    "            if step % 50 == 0: gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Submission saved to {output_file} successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8992374,
     "sourceId": 14115963,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
