{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: E:\\CAFA-6-Protein-Function-Prediction\n",
      "üìÇ Input: E:\\CAFA-6-Protein-Function-Prediction\\input\n",
      "üìÇ Output: E:\\CAFA-6-Protein-Function-Prediction\\output\n",
      "üìÇ Models: E:\\CAFA-6-Protein-Function-Prediction\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH G·ªêC (ROOT) ---\n",
    "# D√πng r'' ƒë·ªÉ Python hi·ªÉu ƒë√¢y l√† ƒë∆∞·ªùng d·∫´n Windows (kh√¥ng b·ªã l·ªói k√Ω t·ª± ƒë·∫∑c bi·ªát)\n",
    "ROOT = r'E:\\CAFA-6-Protein-Function-Prediction'\n",
    "\n",
    "# --- 2. ƒê·ªäNH NGHƒ®A C√ÅC TH∆Ø M·ª§C CON (T·ª± ƒë·ªông n·ªëi ƒëu√¥i) ---\n",
    "# os.path.join gi√∫p n·ªëi ƒë∆∞·ªùng d·∫´n ƒë√∫ng chu·∫©n cho c·∫£ Windows/Linux/Mac\n",
    "INPUT_DIR  = os.path.join(ROOT, 'input')\n",
    "OUTPUT_DIR = os.path.join(ROOT, 'output')\n",
    "MODEL_DIR  = os.path.join(ROOT, 'models')\n",
    "\n",
    "# T·∫°o s·∫µn th∆∞ m·ª•c output v√† models n·∫øu ch∆∞a c√≥ (Tr√°nh l·ªói kh√¥ng l∆∞u ƒë∆∞·ª£c file)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: {ROOT}\")\n",
    "print(f\"üìÇ Input: {INPUT_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìÇ Models: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T16:57:14.185230Z",
     "iopub.status.busy": "2025-12-15T16:57:14.184638Z",
     "iopub.status.idle": "2025-12-15T17:00:50.610288Z",
     "shell.execute_reply": "2025-12-15T17:00:50.609607Z",
     "shell.execute_reply.started": "2025-12-15T16:57:14.185193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locked Random Seed: 42\n",
      "Running on: cuda\n",
      "Train Taxonomy Path: /kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\n",
      "--- 1. Loading Embeddings & Fixing IDs ---\n",
      "Sample Raw Train ID: sp|A0A0C5B5G6|MOTSC_HUMAN\n",
      "Sample Cleaned Train ID: A0A0C5B5G6\n",
      "Train shape: (82404, 1024)\n",
      "--- 2. Processing Taxonomy Features ---\n",
      "Generating Taxon Vectors...\n",
      "   > Found taxonomy for 82403/82404 proteins\n",
      "   > Found taxonomy for 0/224309 proteins\n",
      "--- 3. Processing Labels & Checking Mismatch ---\n",
      "Sample ID in Train Terms: EntryID\n",
      "DEBUG: Total proteins: 82404\n",
      "DEBUG: Matched proteins with labels: 76308\n",
      "\n",
      "--- Starting Ensemble Training (5 Folds) ---\n",
      ">> Fold 1/5\n",
      ">> Fold 2/5\n",
      ">> Fold 3/5\n",
      ">> Fold 4/5\n",
      ">> Fold 5/5\n",
      "\n",
      "--- Running Inference on Test Set ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be00c6f5447e4c9bbf6edd81ec895351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 1:   0%|          | 0/877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4f0c21a5354ee9937e059a521c1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 2:   0%|          | 0/877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c200431721504c3cb8e98189923716ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 3:   0%|          | 0/877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d0e05410c8468eaa8f5a17674860f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 4:   0%|          | 0/877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac3cc5b53f470cb7e5f09caae62f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting Fold 5:   0%|          | 0/877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying GO Structure Propagation ---\n",
      "--- Creating Submission File ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f92a665ad248a5ba926b6150f2c115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224309 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! File submission.tsv is ready in Output tab.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import gc\n",
    "import obonet\n",
    "import networkx as nx\n",
    "import random # Needed for seeding\n",
    "\n",
    "# ==========================================\n",
    "# 0. REPRODUCIBILITY (SEED EVERYTHING)\n",
    "# ==========================================\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for generating random numbers to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # For multi-GPU\n",
    "    # Deterministic algorithms (makes it slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Locked Random Seed: {seed}\")\n",
    "\n",
    "# Apply seed immediately\n",
    "seed_everything(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION PATHS (MATCHING YOUR IMAGE)\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # Root Dir (Main dataset folder on Kaggle)\n",
    "    # Note: Folder name might vary depending on when data was added, check right panel\n",
    "    RAW_DATA_DIR = f'{INPUT_DIR}/cafa-6-protein-function-prediction'\n",
    "    EMBED_DIR = f'{INPUT_DIR}/cafa-6-t5-embeddings'\n",
    "    \n",
    "    # Sub-folder structure (Based on image 2)\n",
    "    TRAIN_DIR = os.path.join(RAW_DATA_DIR, 'Train')\n",
    "    TEST_DIR = os.path.join(RAW_DATA_DIR, 'Test')\n",
    "    \n",
    "    # File Paths\n",
    "    TRAIN_TERMS = os.path.join(TRAIN_DIR, 'train_terms.tsv')\n",
    "    TRAIN_TAXONOMY = os.path.join(TRAIN_DIR, 'train_taxonomy.tsv')\n",
    "    OBO_FILE = os.path.join(TRAIN_DIR, 'go-basic.obo')\n",
    "    \n",
    "    # Test Taxonomy (Crucial for boosting Test set score)\n",
    "    TEST_TAXONOMY = os.path.join(TEST_DIR, 'testsuperset-taxon-list.tsv')\n",
    "    \n",
    "    # Embeddings Paths (Based on image 1)\n",
    "    TRAIN_EMBEDS = os.path.join(EMBED_DIR, 'train_embeds.npy')\n",
    "    TRAIN_IDS = os.path.join(EMBED_DIR, 'train_ids.npy')\n",
    "    TEST_EMBEDS = os.path.join(EMBED_DIR, 'test_embeds.npy')\n",
    "    TEST_IDS = os.path.join(EMBED_DIR, 'test_ids.npy')\n",
    "    \n",
    "    # Hyperparameters\n",
    "    NUM_CLASSES = 1500      # Number of GO Terms (Labels)\n",
    "    BATCH_SIZE = 256        # Large batch for faster GPU run\n",
    "    EPOCHS = 12             # Epochs per fold\n",
    "    LR = 1e-3\n",
    "    N_FOLDS = 5             # Ensemble 5 models\n",
    "    TOP_TAXONS_COUNT = 50   # Top 50 most common species for one-hot\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Running on: {cfg.DEVICE}\")\n",
    "print(f\"Train Taxonomy Path: {cfg.TRAIN_TAXONOMY}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA PROCESSING & TAXONOMY\n",
    "# ==========================================\n",
    "# ==========================================\n",
    "# UPDATE: ID PROCESSING FUNCTION TO FIX N_SAMPLES=0 ERROR\n",
    "# ==========================================\n",
    "\n",
    "def clean_ids(id_list):\n",
    "    \"\"\"\n",
    "    Converts IDs from 'sp|P12345|XYZ' to 'P12345' format if needed.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for x in id_list:\n",
    "        # Convert bytes to string if needed\n",
    "        if isinstance(x, bytes):\n",
    "            x = x.decode('utf-8')\n",
    "        x = str(x).strip()\n",
    "        \n",
    "        # Handle sp|Accession|Name format\n",
    "        if '|' in x:\n",
    "            # Extract middle part (Accession ID)\n",
    "            parts = x.split('|')\n",
    "            if len(parts) >= 2:\n",
    "                cleaned.append(parts[1]) # Gets P12345\n",
    "            else:\n",
    "                cleaned.append(x)\n",
    "        else:\n",
    "            cleaned.append(x)\n",
    "    return np.array(cleaned)\n",
    "\n",
    "def load_and_align_data_fixed():\n",
    "    print(\"--- 1. Loading Embeddings & Fixing IDs ---\")\n",
    "    X_train = np.load(cfg.TRAIN_EMBEDS)\n",
    "    train_ids_raw = np.load(cfg.TRAIN_IDS)\n",
    "    \n",
    "    X_test = np.load(cfg.TEST_EMBEDS)\n",
    "    test_ids_raw = np.load(cfg.TEST_IDS)\n",
    "    \n",
    "    # --- CRITICAL FIX STEP: CLEAN IDS ---\n",
    "    train_ids = clean_ids(train_ids_raw)\n",
    "    test_ids = clean_ids(test_ids_raw)\n",
    "    \n",
    "    # DEBUG: Print to check if IDs match\n",
    "    print(f\"Sample Raw Train ID: {train_ids_raw[0]}\")\n",
    "    print(f\"Sample Cleaned Train ID: {train_ids[0]}\")\n",
    "    print(f\"Train shape: {X_train.shape}\")\n",
    "    \n",
    "    # --- PROCESS TAXONOMY ---\n",
    "    print(\"--- 2. Processing Taxonomy Features ---\")\n",
    "    \n",
    "    # Read Taxonomy file\n",
    "    train_tax_df = pd.read_csv(cfg.TRAIN_TAXONOMY, sep='\\t', dtype=str)\n",
    "    # Map: ProteinID -> TaxonID\n",
    "    # Note: First column of taxonomy must also match cleaned ID\n",
    "    train_tax_map = dict(zip(train_tax_df.iloc[:, 0], train_tax_df.iloc[:, 1]))\n",
    "    \n",
    "    test_tax_df = pd.read_csv(cfg.TEST_TAXONOMY, sep='\\t', dtype=str)\n",
    "    test_tax_map = dict(zip(test_tax_df.iloc[:, 0], test_tax_df.iloc[:, 1]))\n",
    "    \n",
    "    top_taxons = train_tax_df.iloc[:, 1].value_counts().head(cfg.TOP_TAXONS_COUNT).index.tolist()\n",
    "    tax2idx = {t: i for i, t in enumerate(top_taxons)}\n",
    "    \n",
    "    def create_tax_features(ids, tax_map):\n",
    "        feats = np.zeros((len(ids), cfg.TOP_TAXONS_COUNT), dtype=np.float32)\n",
    "        match_count = 0\n",
    "        for i, pid in enumerate(ids):\n",
    "            if pid in tax_map:\n",
    "                match_count += 1\n",
    "                tid = tax_map[pid]\n",
    "                if tid in tax2idx:\n",
    "                    feats[i, tax2idx[tid]] = 1.0\n",
    "        print(f\"   > Found taxonomy for {match_count}/{len(ids)} proteins\")\n",
    "        return feats\n",
    "\n",
    "    print(\"Generating Taxon Vectors...\")\n",
    "    train_tax_feats = create_tax_features(train_ids, train_tax_map)\n",
    "    test_tax_feats = create_tax_features(test_ids, test_tax_map)\n",
    "    \n",
    "    X_train_final = np.concatenate([X_train, train_tax_feats], axis=1)\n",
    "    X_test_final = np.concatenate([X_test, test_tax_feats], axis=1)\n",
    "    \n",
    "    return X_train_final, train_ids, X_test_final, test_ids\n",
    "\n",
    "# --- H√ÄM T√çNH F-MAX ---\n",
    "def calculate_fmax(preds, targets):\n",
    "    thresholds = np.linspace(0.01, 1.0, 50)\n",
    "    fmax = 0.0\n",
    "    best_t = 0.0\n",
    "    targets = targets.astype(int)\n",
    "    \n",
    "    for t in thresholds:\n",
    "        p_cut = (preds >= t).astype(int)\n",
    "        tp = (p_cut * targets).sum(axis=1)\n",
    "        fp = (p_cut * (1 - targets)).sum(axis=1)\n",
    "        fn = ((1 - p_cut) * targets).sum(axis=1)\n",
    "        \n",
    "        precision = np.divide(tp, (tp + fp), out=np.zeros_like(tp, dtype=float), where=(tp + fp) != 0)\n",
    "        recall = np.divide(tp, (tp + fn), out=np.zeros_like(tp, dtype=float), where=(tp + fn) != 0)\n",
    "        \n",
    "        p_avg = precision.mean()\n",
    "        r_avg = recall.mean()\n",
    "        \n",
    "        if (p_avg + r_avg) > 0:\n",
    "            f1 = 2 * p_avg * r_avg / (p_avg + r_avg)\n",
    "        else:\n",
    "            f1 = 0\n",
    "            \n",
    "        if f1 > fmax:\n",
    "            fmax = f1\n",
    "            best_t = t\n",
    "            \n",
    "    return fmax, best_t\n",
    "# ==========================================\n",
    "# 3. MODEL: RESNET (Deep & Residual)\n",
    "# ==========================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.net(x)) # Skip Connection\n",
    "\n",
    "class AdvancedModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # Input Projection: Project vector (1074) to higher dimension\n",
    "        self.entry = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # Deep Residual Layers\n",
    "        self.blocks = nn.Sequential(\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024),\n",
    "            ResidualBlock(1024)\n",
    "        )\n",
    "        self.head = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.entry(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING UTILS\n",
    "# ==========================================\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float() if Y is not None else None\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): \n",
    "        return (self.X[i], self.Y[i]) if self.Y is not None else self.X[i]\n",
    "\n",
    "def train_one_fold(fold, model, train_loader, val_loader):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    patience = 3 \n",
    "    \n",
    "    print(f\"--- FOLD {fold+1} STARTED ---\")\n",
    "    \n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        model.train()\n",
    "        t_loss = 0\n",
    "        # Train loop\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(cfg.DEVICE), y.to(cfg.DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t_loss += loss.item()\n",
    "            \n",
    "        # Validation loop (C·∫¨P NH·∫¨T ƒê·ªÇ T√çNH F-MAX)\n",
    "        model.eval()\n",
    "        v_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(cfg.DEVICE), y.to(cfg.DEVICE)\n",
    "                logits = model(x)\n",
    "                v_loss += criterion(logits, y).item()\n",
    "                \n",
    "                # L∆∞u l·∫°i d·ª± ƒëo√°n ƒë·ªÉ t√≠nh F-max\n",
    "                # Ph·∫£i d√πng Sigmoid v√¨ output c·ªßa model l√† Logits\n",
    "                val_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_targets.append(y.cpu().numpy())\n",
    "        \n",
    "        avg_v_loss = v_loss / len(val_loader)\n",
    "        \n",
    "        # G·ªôp c√°c batch l·∫°i v√† t√≠nh F-max\n",
    "        val_preds = np.concatenate(val_preds)\n",
    "        val_targets = np.concatenate(val_targets)\n",
    "        fmax_score, best_t = calculate_fmax(val_preds, val_targets)\n",
    "        \n",
    "        scheduler.step(avg_v_loss)\n",
    "        \n",
    "        # In k·∫øt qu·∫£ ra m√†n h√¨nh\n",
    "        print(f\"Ep {epoch+1}: Loss={avg_v_loss:.4f} | F-max={fmax_score:.4f} (threshold={best_t:.2f})\")\n",
    "        \n",
    "        # Save Best Model\n",
    "        if avg_v_loss < best_loss:\n",
    "            best_loss = avg_v_loss\n",
    "            torch.save(model.state_dict(), f'model_fold_{fold}.pth')\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            \n",
    "        if early_stop_count >= patience:\n",
    "            print(f\"  >> Fold {fold+1}: Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return best_loss\n",
    "\n",
    "# ==========================================\n",
    "# 5. MAIN PIPELINE (ENSEMBLE 5-FOLD)\n",
    "# ==========================================\n",
    "\n",
    "# --- LOAD DATA WITH NEW FUNCTION ---\n",
    "X_train_full, train_ids_full, X_test_full, test_ids_full = load_and_align_data_fixed()\n",
    "\n",
    "print(\"--- 3. Processing Labels & Checking Mismatch ---\")\n",
    "df = pd.read_csv(cfg.TRAIN_TERMS, sep='\\t', names=['EntryID', 'term', 'aspect'])\n",
    "print(f\"Sample ID in Train Terms: {df['EntryID'].iloc[0]}\") # Debug to check ID format in labels file\n",
    "\n",
    "# Get Top Labels\n",
    "top_terms = df['term'].value_counts().head(cfg.NUM_CLASSES).index.tolist()\n",
    "term2idx = {t: i for i, t in enumerate(top_terms)}\n",
    "\n",
    "# Map ID -> Set of Terms\n",
    "id_to_terms = df[df['term'].isin(top_terms)].groupby('EntryID')['term'].apply(set).to_dict()\n",
    "\n",
    "# Create Matrix Y\n",
    "Y_full = np.zeros((len(train_ids_full), cfg.NUM_CLASSES), dtype=np.float32)\n",
    "valid_mask = []\n",
    "\n",
    "for i, pid in enumerate(train_ids_full):\n",
    "    if pid in id_to_terms:\n",
    "        valid_mask.append(True)\n",
    "        for term in id_to_terms[pid]:\n",
    "            Y_full[i, term2idx[term]] = 1.0\n",
    "    else:\n",
    "        valid_mask.append(False)\n",
    "\n",
    "valid_mask = np.array(valid_mask)\n",
    "print(f\"DEBUG: Total proteins: {len(train_ids_full)}\")\n",
    "print(f\"DEBUG: Matched proteins with labels: {sum(valid_mask)}\")\n",
    "\n",
    "if sum(valid_mask) == 0:\n",
    "    raise ValueError(\"CRITICAL ERROR: No IDs matched! Check the 'Sample Cleaned Train ID' vs 'Sample ID in Train Terms' printed above.\")\n",
    "\n",
    "# Keep only matched data\n",
    "X_train_clean = X_train_full[valid_mask]\n",
    "Y_clean = Y_full[valid_mask]\n",
    "\n",
    "# --- CONTINUE TRAINING LOOP ---\n",
    "kf = KFold(n_splits=cfg.N_FOLDS, shuffle=True, random_state=42)\n",
    "input_dim = X_train_clean.shape[1]\n",
    "\n",
    "print(f\"\\n--- Starting Ensemble Training ({cfg.N_FOLDS} Folds) ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_clean)):\n",
    "    print(f\">> Fold {fold+1}/{cfg.N_FOLDS}\")\n",
    "    \n",
    "    train_ds = ProteinDataset(X_train_clean[train_idx], Y_clean[train_idx])\n",
    "    val_ds = ProteinDataset(X_train_clean[val_idx], Y_clean[val_idx])\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.BATCH_SIZE)\n",
    "    \n",
    "    model = AdvancedModel(input_dim, cfg.NUM_CLASSES).to(cfg.DEVICE)\n",
    "    train_one_fold(fold, model, train_loader, val_loader)\n",
    "    \n",
    "    del model, train_loader, val_loader, train_ds, val_ds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# C. INFERENCE & AVERAGING\n",
    "print(\"\\n--- Running Inference on Test Set ---\")\n",
    "test_loader = DataLoader(ProteinDataset(X_test_full), batch_size=cfg.BATCH_SIZE)\n",
    "final_preds = np.zeros((len(test_ids_full), cfg.NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "for fold in range(cfg.N_FOLDS):\n",
    "    model = AdvancedModel(input_dim, cfg.NUM_CLASSES).to(cfg.DEVICE)\n",
    "    model.load_state_dict(torch.load(f'model_fold_{fold}.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(test_loader, desc=f\"Predicting Fold {fold+1}\"):\n",
    "            x = x.to(cfg.DEVICE)\n",
    "            logits = model(x)\n",
    "            fold_preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            \n",
    "    final_preds += np.vstack(fold_preds)\n",
    "\n",
    "final_preds /= cfg.N_FOLDS # Average results of 5 folds\n",
    "\n",
    "# D. GO PROPAGATION (SCORE HACK)\n",
    "print(\"--- Applying GO Structure Propagation ---\")\n",
    "# Load GO Graph\n",
    "go_graph = obonet.read_obo(cfg.OBO_FILE)\n",
    "term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "\n",
    "# Find parents for each term\n",
    "parent_map = {}\n",
    "valid_set = set(top_terms)\n",
    "for term in top_terms:\n",
    "    if term in go_graph:\n",
    "        parents = list(go_graph.successors(term))\n",
    "        for p in parents:\n",
    "            if p in valid_set:\n",
    "                if p not in parent_map: parent_map[p] = []\n",
    "                parent_map[p].append(term)\n",
    "\n",
    "# Propagate: Score Parent = Max(Score Parent, Max(Score Child))\n",
    "# Run 2 passes to ensure propagation up the hierarchy\n",
    "for _ in range(2):\n",
    "    for parent, children in parent_map.items():\n",
    "        if children:\n",
    "            p_idx = term_to_idx[parent]\n",
    "            c_indices = [term_to_idx[c] for c in children]\n",
    "            max_child_score = final_preds[:, c_indices].max(axis=1)\n",
    "            final_preds[:, p_idx] = np.maximum(final_preds[:, p_idx], max_child_score)\n",
    "\n",
    "# E. SUBMISSION FILE\n",
    "print(\"--- Creating Submission File ---\")\n",
    "submission_rows = []\n",
    "threshold = 0.005 # Low threshold to keep more results (Kaggle metric calculates weighted F1)\n",
    "\n",
    "for i, pid in enumerate(tqdm(test_ids_full)):\n",
    "    # Use numpy where for fast filtering\n",
    "    idxs = np.where(final_preds[i] > threshold)[0]\n",
    "    for idx in idxs:\n",
    "        term = top_terms[idx]\n",
    "        score = final_preds[i, idx]\n",
    "        submission_rows.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "\n",
    "with open(f'{OUTPUT_DIR}submission_dl.tsv', 'w') as f:\n",
    "    # f.write(\"ProteinID\\tGO_ID\\tScore\\n\") # Uncomment if header is required\n",
    "    f.write('\\n'.join(submission_rows))\n",
    "\n",
    "print(\"DONE! File submission_dl.tsv is ready in Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    },
    {
     "datasetId": 8688768,
     "sourceId": 13665986,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8992374,
     "sourceId": 14115963,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
