{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f434cf",
   "metadata": {},
   "source": [
    "# --- 0. CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c489779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: E:\\CAFA-6-Protein-Function-Prediction\n",
      "üìÇ Input: E:\\CAFA-6-Protein-Function-Prediction\\input\n",
      "üìÇ Output: E:\\CAFA-6-Protein-Function-Prediction\\output\n",
      "üìÇ Models: E:\\CAFA-6-Protein-Function-Prediction\\models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. C·∫§U H√åNH G·ªêC (ROOT) ---\n",
    "# D√πng r'' ƒë·ªÉ Python hi·ªÉu ƒë√¢y l√† ƒë∆∞·ªùng d·∫´n Windows (kh√¥ng b·ªã l·ªói k√Ω t·ª± ƒë·∫∑c bi·ªát)\n",
    "ROOT = r'E:\\CAFA-6-Protein-Function-Prediction'\n",
    "\n",
    "# --- 2. ƒê·ªäNH NGHƒ®A C√ÅC TH∆Ø M·ª§C CON (T·ª± ƒë·ªông n·ªëi ƒëu√¥i) ---\n",
    "# os.path.join gi√∫p n·ªëi ƒë∆∞·ªùng d·∫´n ƒë√∫ng chu·∫©n cho c·∫£ Windows/Linux/Mac\n",
    "INPUT_DIR  = os.path.join(ROOT, 'input')\n",
    "OUTPUT_DIR = os.path.join(ROOT, 'output')\n",
    "MODEL_DIR  = os.path.join(ROOT, 'models')\n",
    "\n",
    "# T·∫°o s·∫µn th∆∞ m·ª•c output v√† models n·∫øu ch∆∞a c√≥ (Tr√°nh l·ªói kh√¥ng l∆∞u ƒë∆∞·ª£c file)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ c·∫•u h√¨nh ROOT: {ROOT}\")\n",
    "print(f\"üìÇ Input: {INPUT_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìÇ Models: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0257b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# TORCH MODULES\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cd10f",
   "metadata": {},
   "source": [
    "# --- 1. SETUP & CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f92e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫£i c·∫•u h√¨nh Config.\n",
      "üîπ Device: cuda\n",
      "üîπ Batch Size: 16\n",
      "üîπ Max Sequence Length: 1024\n"
     ]
    }
   ],
   "source": [
    "# --- C·∫§U H√åNH H·ªÜ TH·ªêNG & MODEL (UPDATED FOR 1D-CNN) ---\n",
    "\n",
    "class config:\n",
    "    # 1. ƒê∆∞·ªùng d·∫´n (S·ª≠ d·ª•ng ROOT ƒë√£ ƒë·ªãnh nghƒ©a ·ªü cell ƒë·∫ßu ti√™n)\n",
    "    MAIN_DIR = os.path.join(ROOT, \"input\", \"cafa-6-protein-function-prediction\")\n",
    "    \n",
    "    # ƒê∆∞·ªùng d·∫´n c·ª• th·ªÉ ƒë·∫øn file Fasta (Ti·ªán cho Dataset g·ªçi)\n",
    "    TRAIN_FASTA = os.path.join(MAIN_DIR, \"Train\", \"train_sequences.fasta\")\n",
    "    TRAIN_LABELS = os.path.join(MAIN_DIR, \"Train\", \"train_terms.tsv\")\n",
    "    TEST_FASTA = os.path.join(MAIN_DIR, \"Test\", \"testsuperset.fasta\")\n",
    "    \n",
    "    # 2. Tham s·ªë Model (D√†nh ri√™ng cho x·ª≠ l√Ω chu·ªói)\n",
    "    num_labels = 500      # S·ªë l∆∞·ª£ng GO Term c·∫ßn d·ª± ƒëo√°n\n",
    "    vocab_size = 25       # S·ªë l∆∞·ª£ng k√Ω t·ª± Amino Acid (20 chu·∫©n + v√†i k√Ω t·ª± l·∫°/padding)\n",
    "    embedding_dim = 128   # K√≠ch th∆∞·ªõc vector nh√∫ng n·ªôi b·ªô (Internal Embedding c·ªßa CNN)\n",
    "    max_len = 1024        # ƒê·ªô d√†i chu·ªói t·ªëi ƒëa (C·∫Øt ho·∫∑c Pad v·ªÅ ƒë·ªô d√†i n√†y)\n",
    "    \n",
    "    # 3. Tham s·ªë Training\n",
    "    n_epochs = 8\n",
    "    \n",
    "    # Batch size = 16 (T·ªëi ∆∞u cho card MX550 2GB/4GB VRAM)\n",
    "    # N·∫øu ch·∫°y th·ª≠ th·∫•y m∆∞·ª£t, c√≥ th·ªÉ tƒÉng l√™n 32. N·∫øu l·ªói OOM th√¨ gi·∫£m xu·ªëng 8.\n",
    "    batch_size = 16       \n",
    "    \n",
    "    # Learning rate gi·∫£m nh·∫π ƒë·ªÉ model h·ªçc ·ªïn ƒë·ªãnh v·ªõi batch size nh·ªè\n",
    "    lr = 0.0005           \n",
    "    \n",
    "    # T·ª± ƒë·ªông ch·ªçn GPU n·∫øu c√≥\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫£i c·∫•u h√¨nh Config.\")\n",
    "print(f\"üîπ Device: {config.device}\")\n",
    "print(f\"üîπ Batch Size: {config.batch_size}\")\n",
    "print(f\"üîπ Max Sequence Length: {config.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf9282",
   "metadata": {},
   "source": [
    "# --- 2. DATA LOADING: PROTEIN SEQUENCE DATASET ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a806b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "# C·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán biopython n·∫øu ch∆∞a c√≥: !pip install biopython\n",
    "from Bio import SeqIO\n",
    "\n",
    "# 1. T·∫°o b·ªô t·ª´ ƒëi·ªÉn (Vocab) cho Axit Amin\n",
    "AA_VOCAB = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_int = {aa: i+1 for i, aa in enumerate(AA_VOCAB)} # 1-20, 0 l√† padding\n",
    "\n",
    "def encode_seq(seq, max_len=1024):\n",
    "    seq = str(seq)\n",
    "    encoded = [aa_to_int.get(aa, 0) for aa in seq]\n",
    "    if len(encoded) > max_len:\n",
    "        return encoded[:max_len]\n",
    "    else:\n",
    "        return encoded + [0] * (max_len - len(encoded))\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "    def __init__(self, datatype, fasta_file, targets_file=None):\n",
    "        self.datatype = datatype\n",
    "        self.data = []\n",
    "        \n",
    "        # ƒê·ªçc file FASTA (Sequence th√¥)\n",
    "        print(f\"Loading sequences from {fasta_file}...\")\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            self.data.append({\n",
    "                \"id\": record.id,\n",
    "                \"seq\": encode_seq(record.seq)\n",
    "            })\n",
    "            \n",
    "        self.df = pd.DataFrame(self.data)\n",
    "        \n",
    "        # N·∫øu l√† Train th√¨ load th√™m nh√£n (Targets)\n",
    "        if datatype == \"train\" and targets_file:\n",
    "            print(\"Loading targets...\")\n",
    "            labels = np.load(targets_file)\n",
    "            # Gi·∫£ s·ª≠ th·ª© t·ª± file npy kh·ªõp v·ªõi fasta (c·∫ßn c·∫©n th·∫≠n ƒëo·∫°n n√†y trong th·ª±c t·∫ø)\n",
    "            # ƒê·ªÉ ƒë∆°n gi·∫£n cho starter, ta g√°n tr·ª±c ti·∫øp\n",
    "            self.labels = labels[:len(self.df)] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Input l√† chu·ªói s·ªë nguy√™n (VD: [1, 5, 2...])\n",
    "        seq_tensor = torch.tensor(self.df.iloc[index][\"seq\"], dtype=torch.long)\n",
    "        \n",
    "        if self.datatype == \"train\":\n",
    "            target = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "            return seq_tensor, target\n",
    "        else:\n",
    "            return seq_tensor, self.df.iloc[index][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1eb28",
   "metadata": {},
   "source": [
    "# --- 3. MODEL ARCHITECTURE: 1D CONVOLUTIONAL NEURAL NETWORK (CNN) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce98a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size=21, embed_dim=128):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # L·ªõp n√†y s·∫Ω t·ª± h·ªçc ƒë·∫∑c tr∆∞ng t·ª´ con s·ªë (Axit amin)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # CNN qu√©t tr√™n c√°c vector v·ª´a h·ªçc ƒë∆∞·ª£c\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=9, padding=4),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveMaxPool1d(1), # L·∫•y ƒë·∫∑c tr∆∞ng m·∫°nh nh·∫•t\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len] -> embed: [batch, seq_len, 128]\n",
    "        x = self.embedding(x)\n",
    "        # ƒê·ªïi chi·ªÅu cho Conv1D: [batch, 128, seq_len]\n",
    "        x = x.permute(0, 2, 1) \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd18a9f",
   "metadata": {},
   "source": [
    "# --- 4. MODEL TRAINING (S·ª¨A ƒê·ªîI CHO SEQUENCE CNN) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Dataset...\n",
      "Loading sequences from E:\\CAFA-6-Protein-Function-Prediction\\input\\cafa-6-protein-function-prediction\\Train\\train_sequences.fasta...\n",
      "Loading targets...\n",
      "STARTING TRAINING on cuda...\n",
      "\n",
      "EPOCH 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [00:49<00:00, 93.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:02<00:00, 242.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0459 (Threshold: 0.10)\n",
      "  ‚≠ê New best model saved!\n",
      "\n",
      "EPOCH 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [03:50<00:00, 20.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0498 (Threshold: 0.10)\n",
      "  ‚≠ê New best model saved!\n",
      "\n",
      "EPOCH 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [06:59<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0534 (Threshold: 0.10)\n",
      "  ‚≠ê New best model saved!\n",
      "\n",
      "EPOCH 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [07:03<00:00, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0494 (Threshold: 0.10)\n",
      "\n",
      "EPOCH 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [06:57<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0515 (Threshold: 0.10)\n",
      "\n",
      "EPOCH 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [06:56<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0456 (Threshold: 0.10)\n",
      "\n",
      "EPOCH 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [07:00<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0492 (Threshold: 0.10)\n",
      "\n",
      "EPOCH 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4636/4636 [06:58<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 516/516 [00:12<00:00, 40.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val F1: 0.0532 (Threshold: 0.10)\n",
      "\n",
      "TRAINING FINISHED\n",
      "Highest Validation F1-Score: 0.0534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18640\\2864222538.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{ROOT}/models/cnn_best.pth\", map_location=config.device)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. MODEL TRAINING (S·ª¨A ƒê·ªîI CHO SEQUENCE CNN) ---\n",
    "\n",
    "def train_model(train_size=0.9):\n",
    "    \n",
    "    # 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n file (ƒê·∫£m b·∫£o b·∫°n ƒë√£ Add dataset train_targets_top500)\n",
    "    targets_path = F\"{ROOT}/input/train_targets_top500/train_targets_top500.npy\"\n",
    "    \n",
    "    # 2. Kh·ªüi t·∫°o Dataset m·ªõi (ƒê·ªçc t·ª´ Fasta)\n",
    "    print(\"Initializing Dataset...\")\n",
    "    train_dataset = ProteinSequenceDataset(\n",
    "        datatype=\"train\", \n",
    "        fasta_file=Path(config.TRAIN_FASTA), \n",
    "        targets_file=targets_path\n",
    "    )\n",
    "    \n",
    "    # 3. Chia t·∫≠p Train/Val\n",
    "    train_len = int(len(train_dataset) * train_size)\n",
    "    val_len = len(train_dataset) - train_len\n",
    "    train_set, val_set = random_split(train_dataset, [train_len, val_len])\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # 4. Kh·ªüi t·∫°o Model CNN (Kh√¥ng c·∫ßn input_dim t·ª´ ESM2 n·ªØa)\n",
    "    # vocab_size=21 (20 axit amin + 1 padding), embed_dim=128 (t√πy ch·ªçn)\n",
    "    model = CNN1D(num_classes=config.num_labels, vocab_size=21, embed_dim=128).to(config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Metric F1\n",
    "    f1_metric = MultilabelF1Score(num_labels=config.num_labels, average='macro').to(config.device)\n",
    "\n",
    "    print(f\"STARTING TRAINING on {config.device}...\")\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_threshold = 0.2 # Ng∆∞·ª°ng m·∫∑c ƒë·ªãnh ban ƒë·∫ßu\n",
    "\n",
    "    for epoch in range(config.n_epochs):\n",
    "        print(f\"\\nEPOCH {epoch+1}/{config.n_epochs}\")\n",
    "        \n",
    "        # --- TRAIN PHASE ---\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for seqs, targets in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            seqs, targets = seqs.to(config.device), targets.to(config.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds_logits = model(seqs)\n",
    "            loss = loss_fn(preds_logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"  Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        all_val_preds = []\n",
    "        all_val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for seqs, targets in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                seqs, targets = seqs.to(config.device), targets.to(config.device)\n",
    "                preds_logits = model(seqs)\n",
    "                \n",
    "                all_val_preds.append(torch.sigmoid(preds_logits))\n",
    "                all_val_targets.append(targets)\n",
    "\n",
    "        all_val_preds = torch.cat(all_val_preds)\n",
    "        all_val_targets = torch.cat(all_val_targets)\n",
    "\n",
    "        # T√¨m ng∆∞·ª°ng (Threshold) t·ªëi ∆∞u\n",
    "        best_f1_epoch = 0\n",
    "        best_thresh_epoch = 0\n",
    "        thresholds = np.arange(0.1, 0.51, 0.05)\n",
    "        \n",
    "        # Chuy·ªÉn metric v·ªÅ CPU ƒë·ªÉ tr√°nh l·ªói b·ªô nh·ªõ n·∫øu GPU y·∫øu\n",
    "        f1_metric_cpu = f1_metric.to('cpu')\n",
    "        all_val_preds_cpu = all_val_preds.to('cpu')\n",
    "        all_val_targets_cpu = all_val_targets.to('cpu').int()\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            f1_metric_cpu.threshold = thresh\n",
    "            f1 = f1_metric_cpu(all_val_preds_cpu, all_val_targets_cpu)\n",
    "            if f1 > best_f1_epoch:\n",
    "                best_f1_epoch = f1\n",
    "                best_thresh_epoch = thresh\n",
    "\n",
    "        print(f\"  Val F1: {best_f1_epoch:.4f} (Threshold: {best_thresh_epoch:.2f})\")\n",
    "\n",
    "        scheduler.step(best_f1_epoch)\n",
    "\n",
    "        if best_f1_epoch > best_val_f1:\n",
    "            best_val_f1 = best_f1_epoch\n",
    "            best_threshold = best_thresh_epoch\n",
    "            torch.save(model.state_dict(), f\"{ROOT}/models/cnn_best.pth\")\n",
    "            print(f\"  ‚≠ê New best model saved!\")\n",
    "\n",
    "    print(\"\\nTRAINING FINISHED\")\n",
    "    print(f\"Highest Validation F1-Score: {best_val_f1:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{ROOT}/models/cnn_best.pth\", map_location=config.device, weights_only=True)\n",
    "    )\n",
    "    return model, best_threshold\n",
    "\n",
    "# G·ªçi h√†m train (Kh√¥ng c·∫ßn truy·ªÅn tham s·ªë ESM2 n·ªØa)\n",
    "cnn_model, best_threshold = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa35ed",
   "metadata": {},
   "source": [
    "# --- 5. GENERATING PREDICTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73068fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Data from: E:\\CAFA-6-Protein-Function-Prediction\\input\\cafa-6-protein-function-prediction/Test/testsuperset.fasta\n",
      "Loading sequences from E:\\CAFA-6-Protein-Function-Prediction\\input\\cafa-6-protein-function-prediction/Test/testsuperset.fasta...\n",
      "\n",
      "GENERATING PREDICTIONS FOR THE TEST SET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14020/14020 [07:43<00:00, 30.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. GENERATING PREDICTIONS (UPDATED FOR SEQUENCE CNN) ---\n",
    "\n",
    "def predict(model, threshold):\n",
    "    # 1. C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n file Test (File Sequence th√¥)\n",
    "    # L∆∞u √Ω: Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n n√†y n·∫øu b·∫°n l∆∞u ·ªü ch·ªó kh√°c\n",
    "    test_fasta_path = f\"{config.MAIN_DIR}/Test/testsuperset.fasta\"\n",
    "    \n",
    "    print(f\"Loading Test Data from: {test_fasta_path}\")\n",
    "    \n",
    "    # 2. Kh·ªüi t·∫°o Dataset (S·ª≠ d·ª•ng class m·ªõi ƒë√£ s·ª≠a ·ªü tr√™n)\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", fasta_file=test_fasta_path)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 3. L·∫•y danh s√°ch t√™n nh√£n (GO Terms)\n",
    "    # C·∫ßn load l·∫°i file train_terms ƒë·ªÉ bi·∫øt c·ªôt 0 l√† nh√£n g√¨, c·ªôt 1 l√† nh√£n g√¨...\n",
    "    labels_path = f\"{config.MAIN_DIR}/Train/train_terms.tsv\"\n",
    "    labels_df = pd.read_csv(labels_path, sep=\"\\t\")\n",
    "    top_terms = labels_df.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms.head(config.num_labels).index.values\n",
    "    \n",
    "    print(\"\\nGENERATING PREDICTIONS FOR THE TEST SET...\")\n",
    "    \n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for seqs, ids in tqdm(test_dataloader, desc=\"Predicting\"):\n",
    "            seqs = seqs.to(config.device)\n",
    "            \n",
    "            # ƒê∆∞a qua model\n",
    "            preds_logits = model(seqs)\n",
    "            preds_probs = torch.sigmoid(preds_logits).cpu().numpy()\n",
    "            \n",
    "            # L·ªçc k·∫øt qu·∫£ theo ng∆∞·ª°ng (Threshold)\n",
    "            for i, protein_id in enumerate(ids):\n",
    "                protein_probs = preds_probs[i]\n",
    "                # Ch·ªâ l·∫•y nh·ªØng nh√£n c√≥ x√°c su·∫•t > threshold\n",
    "                go_indices = np.where(protein_probs > threshold)[0]\n",
    "                \n",
    "                for idx in go_indices:\n",
    "                    results.append({\n",
    "                        \"Id\": protein_id,\n",
    "                        \"GO term\": labels_names[idx],\n",
    "                        \"Confidence\": protein_probs[idx]\n",
    "                    })\n",
    "    \n",
    "    submission_df = pd.DataFrame(results)\n",
    "    print(\"PREDICTIONS COMPLETE.\")\n",
    "    return submission_df\n",
    "\n",
    "# G·ªçi h√†m predict v·ªõi model CNN v·ª´a train xong\n",
    "# L∆∞u √Ω: Thay 'ems2_model' c≈© b·∫±ng 'cnn_model'\n",
    "submission_df = predict(cnn_model, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df4ee6",
   "metadata": {},
   "source": [
    "# --- 6. SUBMISSION FILE GENERATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82faf33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- B·∫ÆT ƒê·∫¶U T·∫†O FILE SUBMISSION (CNN + BLAST FILL) ---\n",
      "‚úÖ ƒê√£ t√¨m th·∫•y file BLAST t·∫°i: E:\\CAFA-6-Protein-Function-Prediction/input/blast-quick-sprof-zero-pred/submission.tsv\n",
      "‚è≥ ƒêang tr·ªôn k·∫øt qu·∫£ (Merge)...\n",
      "========================================\n",
      "üéâ TH√ÄNH C√îNG! File ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: E:\\CAFA-6-Protein-Function-Prediction/output/submission_2.tsv\n",
      "üìä T·ªïng s·ªë d·ª± ƒëo√°n: 28968486\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N (D·ª±a tr√™n Tree c·ªßa b·∫°n) ---\n",
    "# V√¨ notebook ƒëang ·ªü trong '1_train_models', ta d√πng '../' ƒë·ªÉ ra th∆∞ m·ª•c g·ªëc\n",
    "blast_path = f\"{ROOT}/input/blast-quick-sprof-zero-pred/submission.tsv\"\n",
    "output_path = f\"{ROOT}/output/submission_2.tsv\"\n",
    "\n",
    "print(\"\\n--- B·∫ÆT ƒê·∫¶U T·∫†O FILE SUBMISSION (CNN + BLAST FILL) ---\")\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "if os.path.exists(blast_path):\n",
    "    print(f\"‚úÖ ƒê√£ t√¨m th·∫•y file BLAST t·∫°i: {blast_path}\")\n",
    "    \n",
    "    # Load file BLAST (Header=None v√¨ file n√†y th∆∞·ªùng kh√¥ng c√≥ d√≤ng ti√™u ƒë·ªÅ)\n",
    "    submission_blast = pd.read_csv(blast_path, sep='\\t', header=None, names=['Id', 'GO term', 'Confidence_Blast'])\n",
    "    \n",
    "    # Load k·∫øt qu·∫£ t·ª´ CNN (submission_df l√† bi·∫øn c√≥ s·∫µn t·ª´ cell tr√™n)\n",
    "    # ƒê·ªïi t√™n c·ªôt Confidence th√†nh Confidence_CNN ƒë·ªÉ d·ªÖ ph√¢n bi·ªát\n",
    "    submission_cnn = submission_df.rename(columns={'Confidence': 'Confidence_CNN'})\n",
    "\n",
    "    # --- 3. MERGE V√Ä X·ª¨ L√ù LOGIC ---\n",
    "    print(\"‚è≥ ƒêang tr·ªôn k·∫øt qu·∫£ (Merge)...\")\n",
    "    # Outer join ƒë·ªÉ gi·ªØ l·∫°i t·∫•t c·∫£ c√°c c·∫∑p (Protein, GO Term) t·ª´ c·∫£ 2 ngu·ªìn\n",
    "    subs = pd.merge(submission_cnn, submission_blast, on=['Id', 'GO term'], how='outer')\n",
    "\n",
    "    # LOGIC QUAN TR·ªåNG: \"∆Øu ti√™n CNN, thi·∫øu m·ªõi d√πng BLAST\"\n",
    "    # M·ª•c ti√™u: File n√†y ph·∫£i mang ƒë·∫∑c tr∆∞ng c·ªßa CNN ƒë·ªÉ ph·ª•c v·ª• Ensemble sau n√†y.\n",
    "    # fillna: N·∫øu CNN c√≥ ƒëi·ªÉm -> l·∫•y CNN. N·∫øu CNN l√† NaN (kh√¥ng d·ª± ƒëo√°n) -> l·∫•y BLAST.\n",
    "    subs['Confidence_Final'] = subs['Confidence_CNN'].fillna(subs['Confidence_Blast'])\n",
    "    \n",
    "    # Ch·ªçn c·ªôt cu·ªëi c√πng\n",
    "    final_df = subs[['Id', 'GO term', 'Confidence_Final']]\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file BLAST! Ch·ªâ l∆∞u k·∫øt qu·∫£ thu·∫ßn c·ªßa CNN.\")\n",
    "    final_df = submission_df.rename(columns={'Confidence': 'Confidence_Final'})\n",
    "\n",
    "# --- 4. L∆ØU FILE ---\n",
    "# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(f\"{ROOT}/output\", exist_ok=True)\n",
    "\n",
    "# L∆∞u file (kh√¥ng index, kh√¥ng header ƒë·ªÉ ƒë√∫ng format cu·ªôc thi)\n",
    "final_df.to_csv(output_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"üéâ TH√ÄNH C√îNG! File ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}\")\n",
    "print(f\"üìä T·ªïng s·ªë d·ª± ƒëo√°n: {len(final_df)}\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
